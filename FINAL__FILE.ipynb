{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDwe3JiGxYpv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoA3d_rLx49E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import json\n",
        "\n",
        "from utils import convert_to_dotdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWzRXDrcfgPK"
      },
      "outputs": [],
      "source": [
        "with open(\"hyperparams.json\", \"r\") as f:\n",
        "    hyperparams = convert_to_dotdict(json.load(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gymE9fcuxhPS"
      },
      "outputs": [],
      "source": [
        "pth = \"Data/NIFTY50\"\n",
        "all_nifty = list(os.walk(pth))\n",
        "\n",
        "file_list = sorted(all_nifty[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPsClSoF3IDp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adadelta, Adam, SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ6pLSwGxbxC"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDxz3qZJZsv8"
      },
      "source": [
        "## **Dropping the stocks which start much later than 2000 and which end before most of the stocks**\n",
        "\n",
        "While this will reduce the number of stocks to only 50% of the original amount, at least we will not lose out on length along the time axis, which is what would have happened if we had brought all the stocks' timelines to start at exactly the same date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xvlULhG44oR",
        "outputId": "6dbcfd5f-e510-451c-e3ba-d90f6a1e9e7c"
      },
      "outputs": [],
      "source": [
        "file_list_2000 = []\n",
        "\n",
        "for i in range(len(file_list)-1):\n",
        "    try:\n",
        "        p = os.path.join(pth, file_list[i])\n",
        "        df = pd.read_csv(p, parse_dates=['Date'])\n",
        "\n",
        "        if df[\"Date\"][0] == pd.to_datetime('2000-01-03', format='%Y-%m-%d') and len(df) == 5306:\n",
        "            file_list_2000.append(file_list[i])\n",
        "    except Exception as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a3AiaYZ-cJ4",
        "outputId": "ff92e6f3-2101-44d5-dbf5-1c372edd1e7e"
      },
      "outputs": [],
      "source": [
        "# Checking if all the leftover stocks start at the same date in 2000 or not\n",
        "\n",
        "first_date = pd.to_datetime('2000-01-03', format='%Y-%m-%d')\n",
        "\n",
        "for i in range(len(file_list_2000)):\n",
        "\n",
        "    print(f\"Checking file {i}: {file_list_2000[i]}\")\n",
        "    current_file_path = os.path.join(pth, file_list_2000[i])\n",
        "    current_file_df = pd.read_csv(current_file_path, parse_dates=['Date'])\n",
        "    current_first_date = current_file_df[\"Date\"].iloc[0]\n",
        "    assert current_first_date == first_date, f\"File {file_list_2000[i]} does not start on the same date as the first file.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaCmCjZ-KFIw"
      },
      "source": [
        "## **A preliminary visalization of all the stocks we have**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvavr3m-Nsr5"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "teCZxc4tKEVg",
        "outputId": "7efb4bf9-e3f5-47b7-a45e-46032f8b768d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i in range(len(file_list_2000)):\n",
        "\n",
        "    current_file_path = os.path.join(pth, file_list_2000[i])\n",
        "    current_file_df = pd.read_csv(current_file_path, parse_dates=['Date'])\n",
        "    current_close = current_file_df[\"Close\"]\n",
        "    current_close_index = current_file_df[\"Date\"]\n",
        "\n",
        "    # !randomly plot some lines and skip other lines for clarity\n",
        "    z = random.choices([0, 1], weights=[6, 1])\n",
        "    if z[0]:\n",
        "         plt.plot(current_close_index, current_close, label=file_list_2000[i])\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xydQf1l-MA4w"
      },
      "source": [
        "The vertical straight lines are the decreased prices which occurred due to stock splits. For example, INFY stock (Infosys : file_list_2000[12]) split at:<br>\n",
        "2018-09-12 \n",
        "\n",
        "2015-06-25\n",
        "\n",
        "2014-12-08\n",
        "\n",
        "2006-07-18\n",
        "\n",
        "2004-07-07\n",
        "\n",
        "2000-02-15,\n",
        "\n",
        "all of which correspond to vertical lines in its stock price graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXEcV75Z_t6-"
      },
      "source": [
        "## **Using log returns**\n",
        "\n",
        "#### *Why log returns?*\n",
        "\n",
        "$$\\text{logReturn(finalValue, initialValue)} = \\ln\\left(\\dfrac{\\text{finalValue}}{\\text{initialValue}}\\right)$$\n",
        "\n",
        "\n",
        "1.   log returns are additive, unlike percentage returns: $$\\text{logReturn(finalValue, initialValue) = logReturn(finalValue, intermediateValue) + logReturn(intermediateValue, initialValue)}$$\n",
        "2.   log returns represent the rate at which the initial value would turn into the final value in the given time, *if we assume* ***continuous compounding***.\n",
        "3. Since stock prices tend to approximate a lognormal distribution, the log returns turn out to be normally distributed and the normality is preserved due to additivity (normal + normal $\\implies$ normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF3sWFgQ_Bno"
      },
      "outputs": [],
      "source": [
        "dataframes = []\n",
        "\n",
        "for i in range(len(file_list_2000)):\n",
        "    p = os.path.join(pth, file_list_2000[i])\n",
        "    df = pd.read_csv(p)\n",
        "\n",
        "    df = df[[\"Date\", \"Close\"]]\n",
        "    df.loc[:, \"logReturns\"] = np.log(df[\"Close\"]/df[\"Close\"].shift(1))\n",
        "    df.set_index(pd.to_datetime(df[\"Date\"]), inplace=True)\n",
        "    df.drop(columns=[\"Date\", \"Close\"], inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    dataframes.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "mPl7-H69EaRv",
        "outputId": "240db836-f23e-4b23-a2ef-efe71a4830f7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "sns.kdeplot(dataframes[0][\"logReturns\"], ax=ax[0, 0], fill=True, color=\"r\")\n",
        "ax[0, 0].set_title(\"KDE Plot 1\")\n",
        "\n",
        "sns.kdeplot(dataframes[8][\"logReturns\"], ax=ax[0, 1], fill=True, color=\"g\")\n",
        "ax[0, 1].set_title(\"KDE Plot 2\")\n",
        "\n",
        "sns.kdeplot(dataframes[16][\"logReturns\"], ax=ax[1, 0], fill=True, color=\"b\")\n",
        "ax[1, 0].set_title(\"KDE Plot 3\")\n",
        "\n",
        "sns.kdeplot(dataframes[24][\"logReturns\"], ax=ax[1, 1], fill=True, color=\"m\")\n",
        "ax[1, 1].set_title(\"KDE Plot 4\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfJyXmKoGmdS"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "VI2BAHJbEr2z",
        "outputId": "e075d918-5357-44e7-93c1-bf9176fc3185"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "fig.suptitle(\"Looks normal to me!\")\n",
        "\n",
        "stats.probplot(dataframes[0][\"logReturns\"], dist=\"norm\", plot=ax[0,0])\n",
        "stats.probplot(dataframes[8][\"logReturns\"], dist=\"norm\", plot=ax[0, 1])\n",
        "stats.probplot(dataframes[16][\"logReturns\"], dist=\"norm\", plot=ax[1, 0])\n",
        "stats.probplot(dataframes[24][\"logReturns\"], dist=\"norm\", plot=ax[1, 1])\n",
        "\n",
        "for i, j in [(x, y) for x in range(2) for y in range(2)]:\n",
        "    ax[i, j].get_lines()[1].set_color('r')\n",
        "    ax[i, j].get_lines()[1].set_linestyle('--')\n",
        "\n",
        "    ax[i, j].get_lines()[0].set_color('k')\n",
        "    ax[i, j].get_lines()[0].set_marker('x')\n",
        "    ax[i, j].get_lines()[0].set_markersize(4)\n",
        "\n",
        "    ax[i, j].set_title(f\"Q-Q Plot {2**(i)*i + j + 1}\")\n",
        "    ax[i, j].set_xlabel('Theoretical Quantiles')\n",
        "    ax[i, j].set_ylabel('Sample Quantiles')\n",
        "    ax[i, j].set_xlim(-4, 4)\n",
        "    ax[i, j].set_ylim(-0.3, 0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "bJ_lFXfzO7VL",
        "outputId": "fed2dbef-55d8-4b16-f87f-fb20f9d03f88"
      },
      "outputs": [],
      "source": [
        "i = np.random.randint(len(dataframes))\n",
        "plt.plot(dataframes[i][\"logReturns\"])\n",
        "plt.title(f\"{file_list_2000[i]} log Returns\")\n",
        "plt.figtext(0.5, -0.05, \"The log returns look pretty much stationary except for sudden areas of large variability which correspond to the stock splits\",\n",
        "            wrap=True, horizontalalignment='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exoZRVYlSEk"
      },
      "source": [
        "## **But wait, are these actually amenable for factor analysis? And if so, then what is the appropriate number of factors?**\n",
        "\n",
        "***For this, we use the Kaiser-Meyer-Olkin (KMO) test*** : This statistic is a measure of the proportion of variance among variables that might be common variance. The higher the proportion, the higher the KMO-value, the more suited the data is to factor analysis. **Why**? Because the larger the common variance, the larger is the probability that the time behaviours of the cross-sectional data can be explained by the same factors, for every stock.\n",
        "\n",
        "$$KMO = \\frac{\\sum \\sum r_{ij}^2}{\\sum \\sum r_{ij}^2 + \\sum \\sum q_{ij}^2}$$\n",
        "\n",
        "KMO value should be around 0.65 or above to be appropriate for factor analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "ihbQnksA0wQd",
        "outputId": "190b4180-16c9-45a9-b110-f9af9cef87e3"
      },
      "outputs": [],
      "source": [
        "allLogReturns = pd.DataFrame()\n",
        "\n",
        "for i in range(len(dataframes)):\n",
        "    allLogReturns = pd.concat([allLogReturns, dataframes[i][\"logReturns\"]], axis=1)\n",
        "\n",
        "allLogReturns.columns = [f'logReturns{i}' for i in range(1, 26)]\n",
        "\n",
        "corr_mat = allLogReturns.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_mat, annot=True,\n",
        "            annot_kws={\"fontsize\":6,\n",
        "                       \"fontweight\":\"extra bold\"})\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFAhXSSG2rwN",
        "outputId": "1ac6c974-e632-43b1-c8ac-d70f50260255"
      },
      "outputs": [],
      "source": [
        "%pip install -q factor-analyzer shutup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnk6ADeM2-zB",
        "outputId": "c4da4944-71e8-4e3c-c618-370d8fe1fa3e"
      },
      "outputs": [],
      "source": [
        "import shutup\n",
        "shutup.please()\n",
        "\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "\n",
        "kmo_all, kmo_model = calculate_kmo(allLogReturns.values)\n",
        "\n",
        "print(f\"KMO Value: {kmo_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QHgpDSy4pBP"
      },
      "source": [
        "So this is an decent example of a dataset that is amenable to factor modelling. But what about the number of factors?\n",
        "\n",
        "\n",
        "1.   **Kaiser-Guttman criterion** : The number of factors to include is equal to the number of eigenvalues of the correlation matrix.\n",
        "2. **Catell's Scree** : Find the elbow in the scree plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "ScSjJu8pDjw_",
        "outputId": "4e41718b-269f-4c89-d00c-05d1839765fc"
      },
      "outputs": [],
      "source": [
        "# Kaiser-Guttman criterion\n",
        "\n",
        "eigenvalues_, eigenvectors = np.linalg.eig(corr_mat)\n",
        "eigenvalues = sorted(eigenvalues_, reverse=True)\n",
        "eigenvalues = list(filter(lambda x: x > 0.97, eigenvalues))\n",
        "\n",
        "print(f\"Number of factors according to the Kaiser Guttmann criterion: {len(eigenvalues)}\\n\\n\\n\")\n",
        "\n",
        "plt.plot(range(len(eigenvalues_)), sorted(eigenvalues_, reverse=True))\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Factor\")\n",
        "plt.ylabel(\"Eigenvalue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **An alternative: trying simply returns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataframes = []\n",
        "\n",
        "for i in range(len(file_list_2000)):\n",
        "    p = os.path.join(pth, file_list_2000[i])\n",
        "    df = pd.read_csv(p)\n",
        "\n",
        "    df = df[[\"Date\", \"Close\"]]\n",
        "    df.set_index(pd.to_datetime(df[\"Date\"]), inplace=True)\n",
        "    df.drop(columns=[\"Date\"], inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    dataframes.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "sns.kdeplot(dataframes[0][\"Close\"], ax=ax[0, 0], fill=True, color=\"r\")\n",
        "ax[0, 0].set_title(\"KDE Plot 1\")\n",
        "\n",
        "sns.kdeplot(dataframes[8][\"Close\"], ax=ax[0, 1], fill=True, color=\"g\")\n",
        "ax[0, 1].set_title(\"KDE Plot 2\")\n",
        "\n",
        "sns.kdeplot(dataframes[16][\"Close\"], ax=ax[1, 0], fill=True, color=\"b\")\n",
        "ax[1, 0].set_title(\"KDE Plot 3\")\n",
        "\n",
        "sns.kdeplot(dataframes[24][\"Close\"], ax=ax[1, 1], fill=True, color=\"m\")\n",
        "ax[1, 1].set_title(\"KDE Plot 4\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "allReturns = pd.DataFrame()\n",
        "\n",
        "for i in range(len(dataframes)):\n",
        "    allReturns = pd.concat([allReturns, dataframes[i][\"Close\"]], axis=1)\n",
        "\n",
        "allReturns.columns = [f'Close{i}' for i in range(1, 26)]\n",
        "\n",
        "corr_mat = allReturns.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_mat, annot=True,\n",
        "            annot_kws={\"fontsize\":6,\n",
        "                       \"fontweight\":\"extra bold\"})\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = np.random.randint(len(dataframes))\n",
        "plt.plot(dataframes[i][\"Close\"])\n",
        "plt.title(f\"{file_list_2000[i]} Close Prices\")\n",
        "# plt.figtext(0.5, -0.05, \"The log returns look pretty much stationary except for sudden areas of large variability which correspond to the stock splits\",\n",
        "            # wrap=True, horizontalalignment='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q factor-analyzer shutup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutup\n",
        "shutup.please()\n",
        "\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "\n",
        "kmo_all, kmo_model = calculate_kmo(allReturns.values)\n",
        "\n",
        "print(f\"KMO Value: {kmo_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So this is an decent example of a dataset that is amenable to factor modelling. But what about the number of factors?\n",
        "\n",
        "\n",
        "1.   **Kaiser-Guttman criterion** : The number of factors to include is equal to the number of eigenvalues of the correlation matrix.\n",
        "2. **Catell's Scree** : Find the elbow in the scree plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaiser-Guttman criterion\n",
        "\n",
        "eigenvalues_, eigenvectors = np.linalg.eig(corr_mat)\n",
        "eigenvalues = sorted(eigenvalues_, reverse=True)\n",
        "eigenvalues = list(filter(lambda x: x > 0.97, eigenvalues))\n",
        "\n",
        "print(f\"Number of factors according to the Kaiser Guttmann criterion: {len(eigenvalues)}\\n\\n\\n\")\n",
        "\n",
        "plt.plot(range(len(eigenvalues_)), sorted(eigenvalues_, reverse=True))\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Factor\")\n",
        "plt.ylabel(\"Eigenvalue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxFldHqtL0Bx"
      },
      "source": [
        "## **Preparing the data and introducing lags of a given window size**\n",
        "\n",
        "Here's what we want our data to look like:\n",
        "\n",
        "\n",
        "\n",
        "![](logo.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdUxaVf7Gtve"
      },
      "outputs": [],
      "source": [
        "def introduce_lags(df, window_size):\n",
        "    new = df\n",
        "    for i in range(1, window_size+1):\n",
        "        new[f\"lag_{i}\"] = df[\"logReturns\"].shift(i)\n",
        "\n",
        "    new.dropna(inplace=True)\n",
        "    return new\n",
        "\n",
        "for df in dataframes:\n",
        "    df = introduce_lags(df, hyperparams.DATA.WINDOW_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def introduce_lags_close(df, window_size):\n",
        "    new = df\n",
        "    for i in range(1, window_size+1):\n",
        "        new[f\"lag_{i}\"] = df[\"Close\"].shift(i)\n",
        "\n",
        "    new.dropna(inplace=True)\n",
        "    return new\n",
        "\n",
        "for df in dataframes:\n",
        "    df = introduce_lags_close(df, hyperparams.DATA.WINDOW_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ4pGhbRt25E"
      },
      "source": [
        "## **Creating a train_test split and loading data to the PyTorch DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "Ioc92Lewt_uW",
        "outputId": "21d23f62-b4e7-4e68-a6d3-6d5b6722c721"
      },
      "outputs": [],
      "source": [
        "dataframes[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vxEeWouwNbf",
        "outputId": "bc129053-8054-4a26-b160-34806c4409a8"
      },
      "outputs": [],
      "source": [
        "tens = []\n",
        "\n",
        "for df in dataframes:\n",
        "    tens.append(df.to_numpy())\n",
        "\n",
        "tens = np.array(tens)\n",
        "'''UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow.\n",
        "    Please consider converting the list to a single numpy.ndarray with numpy.array()\n",
        "    before converting to a tensor.'''\n",
        "\n",
        "tens = torch.tensor(tens)\n",
        "tens = tens.reshape([-1, tens.shape[2], tens.shape[0]])\n",
        "tens.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-4bajw91mWa"
      },
      "outputs": [],
      "source": [
        "X_train = tens[:int(hyperparams.DATA.TRAIN_SIZE * tens.shape[0]), 1: , :]\n",
        "X_test = tens[int(hyperparams.DATA.TRAIN_SIZE * tens.shape[0]):, 1: , :]\n",
        "y_train = tens[:int(hyperparams.DATA.TRAIN_SIZE * tens.shape[0]), 0 , :]\n",
        "y_test = tens[int(hyperparams.DATA.TRAIN_SIZE * tens.shape[0]):, 0 , :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVBg5O3q54im",
        "outputId": "3a888b5a-def4-41bf-f5e3-aa9ac6155983"
      },
      "outputs": [],
      "source": [
        "assert X_train.shape[0] > 0\n",
        "assert X_test.shape[0] > 0\n",
        "assert y_train.shape[0] > 0\n",
        "assert y_test.shape[0] > 0\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO4siYowAvZ7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams.TRAINING.BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyperparams.TRAINING.BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88MUMqN6m1r-"
      },
      "source": [
        "## **Creating the Encoder and Decoder to build the final Variational Recurrent AutoEncoder**\n",
        "\n",
        "This is the \"Factor Network\" mentioned in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, params):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "\n",
        "        self.batch_size = self.params.batch_size\n",
        "        self.num_stocks = int(self.params.num_stocks)\n",
        "        self.num_factors = self.params.num_factors\n",
        "        self.num_layers = self.params.num_layers\n",
        "        self.hidden_size = self.params.hidden_size\n",
        "        self.num_lags = self.params.num_lags\n",
        "        self.dropout = self.params.dropout\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.num_stocks,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=self.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        print(\"Hi\")\n",
        "        self.relu = nn.ReLU()\n",
        "        self.mu = nn.Linear(self.hidden_size, self.num_factors)\n",
        "        self.log_sigma = nn.Linear(self.hidden_size, self.num_factors)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.rnn(x)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        mu = self.mu(out)\n",
        "\n",
        "        log_sigma = self.log_sigma(out)\n",
        "\n",
        "        return mu, log_sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, params):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.params = params\n",
        "\n",
        "        self.batch_size = self.params.batch_size\n",
        "        self.num_stocks = self.params.num_stocks\n",
        "        self.num_factors = self.params.num_factors\n",
        "        self.num_layers = self.params.num_layers\n",
        "        self.hidden_size = self.params.hidden_size\n",
        "        self.num_lags = self.params.num_lags\n",
        "        self.dropout = self.params.dropout\n",
        "\n",
        "        self.tanh_h0 = nn.Tanh()\n",
        "        self.rnn = nn.LSTM(\n",
        "                           input_size = self.num_factors,\n",
        "                           hidden_size = self.hidden_size,\n",
        "                           num_layers = self.num_layers,\n",
        "                           dropout = self.dropout,\n",
        "                           batch_first = True\n",
        "                        )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(self.hidden_size, 128)\n",
        "        self.fc2 = nn.Linear(128, self.hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z = self.tanh_h0(z)\n",
        "\n",
        "        out, _ = self.rnn(z)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.relu(out)\n",
        "        # out = self.fc(out)\n",
        "        # out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkgczspsw9I_"
      },
      "outputs": [],
      "source": [
        "class RVAE(nn.Module):\n",
        "    def __init__(self, encoder_params, decoder_params):\n",
        "        super(RVAE, self).__init__()\n",
        "        self.encoder = Encoder(encoder_params)\n",
        "        self.decoder = Decoder(decoder_params)\n",
        "\n",
        "        self.weight_matrix = nn.Parameter(torch.randn(hyperparams.FACTOR_NETWORK.NUM_FACTORS,\n",
        "                                                      hyperparams.DATA.NUM_STOCKS\n",
        "                                                      ),\n",
        "                                          requires_grad=True)\n",
        "        \n",
        "        self.num_factors = decoder_params.num_factors\n",
        "        self.hidden_size = decoder_params.num_stocks\n",
        "\n",
        "        self.fc1 = nn.Linear(hyperparams.DATA.NUM_STOCKS, hyperparams.FACTOR_NETWORK.NUM_FACTORS)\n",
        "        self.mu = None\n",
        "        self.logvar = None\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def sample(self, mu : list, logvar : list):\n",
        "        assert len(mu) == len(logvar)\n",
        "\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        self.mu = mu.to(dtype=torch.float32)\n",
        "        self.logvar = logvar.to(dtype=torch.float32)\n",
        "        z = self.sample(mu, logvar)\n",
        "        out = self.decoder(z)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        # out = self.sigmoid(out)\n",
        "        \n",
        "        return torch.matmul(out, self.weight_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajFMQX2JTgXy"
      },
      "source": [
        "## **Creating the loss function and the training loop**\n",
        "\n",
        "The loss function is a linear combination of the reconstruction/data-fidelity loss, and the KL divergence-loss between the empirical distribution $q_\\theta(z|x)$ we approximated, versus the assumed prior $p(z) \\sim \\mathcal N(0, 1)$. The loss can be expressed as: $$D_{DF} + D_{KL} = KL[q_\\theta(z|x)∥p(z)] - \\mathbb E_Q[\\log(p_\\phi(x|z))]$$ <br>$$=\\sum_{i=1}^k \\bigg[ \\|x_i - x_{pred}\\|_2^2 + \\frac12\\cdot \\bigg(\\operatorname{tr}(\\Sigma(x_i)) + \\mu(x_i)^\\top\\mu(x_i) - k - \\log\\det(\\Sigma(x_i))\\bigg)\\bigg]$$\n",
        "\n",
        "where $k$ is the hidden dimension.\n",
        "\n",
        "The expression, if we consider $(\\Sigma(x_i))=\\operatorname{diag}(\\sigma^2)$, becomes \n",
        "\n",
        "$$\\displaystyle{\\sum_{i=1}^k \\|x_i - x_{pred}\\|_2^2 + \\frac{k}{2} \\sum_{i=1}^k \\left( \\sigma_i^2 + \\mu_i^2 - 1 - \\log(\\sigma_i^2) \\right).}\n",
        "$$\n",
        "\n",
        "A hyperparameter $\\lambda$ can optionally be added as a weighting factor for the KL-divergence as compared to the reconstruction loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKdrw4A7TmmL"
      },
      "outputs": [],
      "source": [
        "def loss_fn(x_pred : torch.Tensor,\n",
        "            x_true : torch.Tensor,\n",
        "            mu : torch.Tensor,\n",
        "            logvar : torch.Tensor):\n",
        "\n",
        "    assert x_pred.shape == x_true.shape, print(x_pred.shape, x_true.shape)\n",
        "#     print(f\"sd = {torch.mean(torch.sqrt(logvar.exp()))}    mu = {torch.mean(mu)}\")\n",
        "\n",
        "    assert mu.shape == logvar.shape\n",
        "\n",
        "    reconstruction_loss = torch.sum((x_pred - x_true)**2)\n",
        "\n",
        "    KLD = 0.5 * torch.sum(mu.pow(2) + logvar.exp() - 1 - logvar)\n",
        "\n",
        "    lambd = hyperparams.TRAINING.KL_LOSS_WEIGHT\n",
        "\n",
        "    return reconstruction_loss + lambd * KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEWUM-VF43r0"
      },
      "outputs": [],
      "source": [
        "train_history = []\n",
        "test_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhmpoue3eYmL"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, model, optimizer):\n",
        "    print(\"------------EPOCH\", epoch_index, \"----------------\")\n",
        "\n",
        "    running_loss = 0.0\n",
        "    model.train(mode=True)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        inputs, label = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = inputs.to(device=device, dtype=torch.float32)\n",
        "        label = label.to(device=device, dtype=torch.float32)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, label, model.mu, model.logvar)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "\n",
        "    train_history.append(epoch_loss)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz65_P3a8ZkB"
      },
      "outputs": [],
      "source": [
        "encoder_params = convert_to_dotdict({\n",
        "        \"batch_size\": hyperparams.TRAINING.BATCH_SIZE,\n",
        "        \"num_stocks\": hyperparams.DATA.NUM_STOCKS,\n",
        "        \"num_lags\": hyperparams.DATA.WINDOW_SIZE,\n",
        "        \"num_factors\": hyperparams.FACTOR_NETWORK.NUM_FACTORS,\n",
        "        \"hidden_size\": hyperparams.FACTOR_NETWORK.ENCODER.PARAMS.HIDDEN_SIZE,\n",
        "        \"dropout\": hyperparams.FACTOR_NETWORK.ENCODER.PARAMS.DROPOUT_PROB,\n",
        "        \"num_layers\": hyperparams.FACTOR_NETWORK.ENCODER.PARAMS.NUM_LAYERS\n",
        "})\n",
        "\n",
        "decoder_params = convert_to_dotdict({\n",
        "        \"batch_size\": hyperparams.TRAINING.BATCH_SIZE,\n",
        "        \"num_stocks\": hyperparams.DATA.NUM_STOCKS,\n",
        "        \"num_lags\": hyperparams.DATA.WINDOW_SIZE,\n",
        "        \"num_factors\": hyperparams.FACTOR_NETWORK.NUM_FACTORS,\n",
        "        \"hidden_size\": hyperparams.FACTOR_NETWORK.DECODER.PARAMS.HIDDEN_SIZE,\n",
        "        \"dropout\": hyperparams.FACTOR_NETWORK.DECODER.PARAMS.DROPOUT_PROB,\n",
        "        \"num_layers\": hyperparams.FACTOR_NETWORK.DECODER.PARAMS.NUM_LAYERS\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Uwqte6bX-Sl6",
        "outputId": "f1ad2fe8-b20c-4065-bd1b-2181abd5e289"
      },
      "outputs": [],
      "source": [
        "model = RVAE(encoder_params, decoder_params)\n",
        "model.to(device)\n",
        "optimizer = Adam(lr = hyperparams.TRAINING.LEARNING_RATE,\n",
        "                 betas = (0.9, 0.9),\n",
        "                 params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGHQQsC50xSp"
      },
      "outputs": [],
      "source": [
        "for epoch in range(hyperparams.TRAINING.NUM_EPOCHS):\n",
        "    model.train(mode=True)\n",
        "    avg_loss = train_one_epoch(epoch, model, optimizer)\n",
        "\n",
        "    running_test_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for i, data in enumerate(test_dataloader):\n",
        "            test_inputs, test_label = data\n",
        "            test_inputs, test_label = test_inputs.to(dtype=torch.float32), test_label.to(dtype=torch.float32)\n",
        "            test_outputs = model(test_inputs)\n",
        "\n",
        "            test_loss = loss_fn(test_outputs, test_label, model.mu, model.logvar)\n",
        "            running_test_loss += test_loss.item()\n",
        "\n",
        "    avg_test_loss = running_test_loss / len(test_dataloader)\n",
        "    print(f\"Epoch {epoch} Train loss: {avg_loss}, Test loss: {avg_test_loss}\")\n",
        "\n",
        "    test_history.append(avg_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(test_history[1:])\n",
        "plt.plot(train_history[1:])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = torch.tensor(np.array([dataframes[i].iloc[70, 1:] for i in range(25)]).reshape(1, 10, 25)).to(dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot([dataframes[i].iloc[70, 0] for i in range(25)])\n",
        "plt.plot(model(k)[0].detach().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tru = []\n",
        "pred = []\n",
        "with torch.inference_mode():\n",
        "        for i, data in enumerate(test_dataloader):\n",
        "            test_inputs, test_label = data\n",
        "            test_inputs, test_label = test_inputs.to(dtype=torch.float32), test_label.to(dtype=torch.float32)\n",
        "            test_outputs = model(test_inputs)\n",
        "            tru.append(test_label)\n",
        "            pred.append(test_outputs)\n",
        "\n",
        "pred[15][0], tru[15][0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
